ixgbe_ethtool.c:	IXGBE_WRITE_REG(hw, IXGBE_TXDCTL(tx_ring->reg_idx), 0);
ixgbe_ethtool.c:	tx_ring->count = IXGBE_DEFAULT_TXD;
ixgbe_ethtool.c:	tx_ring->queue_index = 0;
ixgbe_ethtool.c:	tx_ring->dev = pci_dev_to_dev(adapter->pdev);
ixgbe_ethtool.c:	tx_ring->netdev = adapter->netdev;
ixgbe_ethtool.c:	tx_ring->reg_idx = adapter->tx_ring[0]->reg_idx;
ixgbe_ethtool.c:	tx_ntc = tx_ring->next_to_clean;
ixgbe_ethtool.c:		tx_buffer = &tx_ring->tx_buffer_info[tx_ntc];
ixgbe_ethtool.c:		if (tx_ntc == tx_ring->count)
ixgbe_ethtool.c:	tx_ring->next_to_clean = tx_ntc;
ixgbe_ethtool.c:	if (rx_ring->count <= tx_ring->count)
ixgbe_ethtool.c:		lc = ((tx_ring->count / 64) * 2) + 1;
ixgbe_main.c:		u8 tc = tx_ring->dcb_tc;
ixgbe_main.c:			clear_bit(__IXGBE_HANG_CHECK_ARMED, &tx_ring->state);
ixgbe_main.c:	u32 tx_done_old = tx_ring->tx_stats.tx_done_old;
ixgbe_main.c:				       &tx_ring->state);
ixgbe_main.c:		tx_ring->tx_stats.tx_done_old = tx_done;
ixgbe_main.c:		clear_bit(__IXGBE_HANG_CHECK_ARMED, &tx_ring->state);
ixgbe_main.c:	unsigned int i = tx_ring->next_to_clean;
ixgbe_main.c:	tx_buffer = &tx_ring->tx_buffer_info[i];
ixgbe_main.c:	i -= tx_ring->count;
ixgbe_main.c:      pr_info("[%s][%d] YHOON, if DD is not set pending work has not been completed : %u\n", __FUNCTION__, __LINE__, tx_ring->next_to_clean);
ixgbe_main.c:		dma_unmap_single(tx_ring->dev,
ixgbe_main.c:				i -= tx_ring->count;
ixgbe_main.c:				tx_buffer = tx_ring->tx_buffer_info;
ixgbe_main.c:				dma_unmap_page(tx_ring->dev,
ixgbe_main.c:			i -= tx_ring->count;
ixgbe_main.c:			tx_buffer = tx_ring->tx_buffer_info;
ixgbe_main.c:	i += tx_ring->count;
ixgbe_main.c:	tx_ring->next_to_clean = i;
ixgbe_main.c:	u64_stats_update_begin(&tx_ring->syncp);
ixgbe_main.c:	tx_ring->stats.bytes += total_bytes;
ixgbe_main.c:	tx_ring->stats.packets += total_packets;
ixgbe_main.c:	u64_stats_update_end(&tx_ring->syncp);
ixgbe_main.c:			tx_ring->queue_index,
ixgbe_main.c:			IXGBE_READ_REG(hw, IXGBE_TDH(tx_ring->reg_idx)),
ixgbe_main.c:			IXGBE_READ_REG(hw, IXGBE_TDT(tx_ring->reg_idx)),
ixgbe_main.c:			tx_ring->next_to_use, i);
ixgbe_main.c:			tx_ring->tx_buffer_info[i].time_stamp, jiffies);
ixgbe_main.c:		       adapter->tx_timeout_count + 1, tx_ring->queue_index);
ixgbe_main.c:			++tx_ring->tx_stats.restart_queue;
ixgbe_main.c:			++tx_ring->tx_stats.restart_queue;
ixgbe_main.c:		txctrl = dca3_get_tag(tx_ring->dev, cpu);
ixgbe_main.c:		reg_offset = IXGBE_DCA_TXCTRL(tx_ring->reg_idx);
ixgbe_main.c:		reg_offset = IXGBE_DCA_TXCTRL_82599(tx_ring->reg_idx);
ixgbe_main.c:	if (!tx_ring->tx_buffer_info)
ixgbe_main.c:	for (i = 0; i < tx_ring->count; i++) {
ixgbe_main.c:		tx_buffer_info = &tx_ring->tx_buffer_info[i];
ixgbe_main.c:	size = sizeof(struct ixgbe_tx_buffer) * tx_ring->count;
ixgbe_main.c:	memset(tx_ring->tx_buffer_info, 0, size);
ixgbe_main.c:	memset(tx_ring->desc, 0, tx_ring->size);
ixgbe_main.c:	struct device *dev = tx_ring->dev;
ixgbe_main.c:	size = sizeof(struct ixgbe_tx_buffer) * tx_ring->count;
ixgbe_main.c:	if (tx_ring->q_vector)
ixgbe_main.c:		numa_node = tx_ring->q_vector->numa_node;
ixgbe_main.c:	tx_ring->tx_buffer_info = vzalloc_node(size, numa_node);
ixgbe_main.c:	if (!tx_ring->tx_buffer_info)
ixgbe_main.c:		tx_ring->tx_buffer_info = vzalloc(size);
ixgbe_main.c:	if (!tx_ring->tx_buffer_info)
ixgbe_main.c:	tx_ring->size = tx_ring->count * sizeof(union ixgbe_adv_tx_desc);
ixgbe_main.c:	tx_ring->size = ALIGN(tx_ring->size, 4096);
ixgbe_main.c:	tx_ring->desc = dma_alloc_coherent(dev,
ixgbe_main.c:					   tx_ring->size,
ixgbe_main.c:					   &tx_ring->dma,
ixgbe_main.c:	if (!tx_ring->desc)
ixgbe_main.c:		tx_ring->desc = dma_alloc_coherent(dev, tx_ring->size,
ixgbe_main.c:						   &tx_ring->dma, GFP_KERNEL);
ixgbe_main.c:	if (!tx_ring->desc)
ixgbe_main.c:	vfree(tx_ring->tx_buffer_info);
ixgbe_main.c:	tx_ring->tx_buffer_info = NULL;
ixgbe_main.c:	vfree(tx_ring->tx_buffer_info);
ixgbe_main.c:	tx_ring->tx_buffer_info = NULL;
ixgbe_main.c:	if (!tx_ring->desc)
ixgbe_main.c:	dma_free_coherent(tx_ring->dev, tx_ring->size,
ixgbe_main.c:			  tx_ring->desc, tx_ring->dma);
ixgbe_main.c:	tx_ring->desc = NULL;
ixgbe_main.c:		restart_queue += tx_ring->tx_stats.restart_queue;
ixgbe_main.c:		tx_busy += tx_ring->tx_stats.tx_busy;
ixgbe_main.c:		bytes += tx_ring->stats.bytes;
ixgbe_main.c:		packets += tx_ring->stats.packets;
ixgbe_main.c:		if (tx_ring->next_to_use != tx_ring->next_to_clean)
ixgbe_main.c:				dev_warn(tx_ring->dev,
ixgbe_main.c:				dev_warn(tx_ring->dev,
ixgbe_main.c:				dev_warn(tx_ring->dev,
ixgbe_main.c:	netif_stop_subqueue(tx_ring->netdev, tx_ring->queue_index);
ixgbe_main.c:	netif_start_subqueue(tx_ring->netdev, tx_ring->queue_index);
ixgbe_main.c:	++tx_ring->tx_stats.restart_queue;
ixgbe_main.c:	u16 i = tx_ring->next_to_use;
ixgbe_main.c:	dma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);
ixgbe_main.c:		if (dma_mapping_error(tx_ring->dev, dma))
ixgbe_main.c:			if (i == tx_ring->count) {
ixgbe_main.c:		if (i == tx_ring->count) {
ixgbe_main.c:		dma = skb_frag_dma_map(tx_ring->dev, frag, 0, size,
ixgbe_main.c:		tx_buffer = &tx_ring->tx_buffer_info[i];
ixgbe_main.c:	if (i == tx_ring->count)
ixgbe_main.c:	tx_ring->next_to_use = i;
ixgbe_main.c:		writel(i, tx_ring->tail);
ixgbe_main.c:	writel(i, tx_ring->tail);
ixgbe_main.c:	dev_err(tx_ring->dev, "TX DMA map failed\n");
ixgbe_main.c:	tx_buffer = &tx_ring->tx_buffer_info[i];
ixgbe_main.c:		tx_buffer = &tx_ring->tx_buffer_info[i];
ixgbe_main.c:			i = tx_ring->count;
ixgbe_main.c:	tx_ring->next_to_use = i;
ixgbe_main.c:		tx_ring->tx_stats.tx_busy++;
ixgbe_main.c:	first = &tx_ring->tx_buffer_info[tx_ring->next_to_use];
ixgbe_main.c:	    (tx_ring->netdev->features & (NETIF_F_FSO | NETIF_F_FCOE_CRC))) {
ixgbe_main.c:	if (test_bit(__IXGBE_TX_FDIR_INIT_DONE, &tx_ring->state))
ixgbe_main.c:	struct ixgbe_adapter *adapter = tx_ring->q_vector->adapter;
ixgbe_main.c:      tx_ring->queue_index,
ixgbe_main.c:      IXGBE_READ_REG(hw, IXGBE_TDH(tx_ring->reg_idx)),
ixgbe_main.c:      IXGBE_READ_REG(hw, IXGBE_TDT(tx_ring->reg_idx)),
ixgbe_main.c:      tx_ring->next_to_use, tx_ring->next_to_clean);
ixgbe_main.c:      tx_ring->tx_buffer_info[tx_ring->next_to_clean].time_stamp, jiffies);
ixgbe_main.c:	qidx = tx_ring->next_to_use;
ixgbe_main.c:  tx_buffer = &tx_ring->tx_buffer_info[qidx];
ixgbe_main.c:	tx_ring->stats.packets += 1;
ixgbe_main.c:	tx_ring->stats.bytes += total_bytes;
ixgbe_main.c:  next_qidx = (qidx + 1) % tx_ring->count;
ixgbe_main.c:	tx_ring->next_to_use = next_qidx;
ixgbe_main.c:	writel(next_qidx, tx_ring->tail);
ixgbe_fcoe.c:		dev_err(tx_ring->dev, "Wrong gso type %d:expecting "
ixgbe_fcoe.c:		dev_warn(tx_ring->dev, "unknown sof = 0x%x\n", sof);
ixgbe_fcoe.c:		dev_warn(tx_ring->dev, "unknown eof = 0x%x\n", eof);
ixgbe_lib.c:	u16 i = tx_ring->next_to_use;
ixgbe_lib.c:	tx_ring->next_to_use = (i < tx_ring->count) ? i : 0;
